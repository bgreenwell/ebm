[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 ebm authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/advanced.html","id":"monotonic-constraints","dir":"Articles","previous_headings":"","what":"Monotonic constraints","title":"Advanced usage","text":"PPERSAUT (contribution level car policies) top predictive feature, ’s also categorical force monotonicity . Technically, PPERSAUT ordinal unreasonable recode integer similar refit model (tree-based models tend differentiate ordered nominal features). Instead, ’ll focus number car policies APERSAUT.  can see relationship mostly decreasing, seems logical. car policies already , less likely purchase another. Hence, may make sense force decreasing monotonicity . can force monotonic constraints via monotone_constraints argument call ebm(), original authors generally recommend forcing monotonicity post-processing graphs instead using isotonic regression. can done calling $monotonize() method fitted EBM object. recommended approach prevents model compensating monotonicity constraints learning non-monotonic effects highly-correlated features. example , post-process graph APERDAUT force negative monotonicity (increasing = FALSE) using isotonic regression. Notice change term contributions APERSAUT fitted model. Additionally, lose uncertainty associated feature outer bagging original model.  ebm package expose 100% functionality available Python, example shows can pretty much anything need interacting directly underlying Python objects (magic happens reticulate). example, reproduce original HTML-based visualization, displayed browser, can following: Details aside, can access data recreate plot (like plot() method ), coercing internal Python plotly object ordered dictionary reticulate automatically convert list. instance, following snippet code extracts main data used generating previous plot.","code":"plot(fit, term = \"APERSAUT\") fit$monotonize(\"APERSAUT\", increasing = FALSE) #> ExplainableBoostingClassifier(early_stopping_tolerance=0, #>                               interaction_smoothing_rounds=100, #>                               learning_rate=0.04, max_leaves=2, min_hessian=0.0, #>                               smoothing_rounds=500) plot(fit, term = \"APERSAUT\") idx <- as.integer(which(fit$term_names_ == \"APERSAUT\") - 1L)  plt <- fit$explain_global()$visualize(idx) plt$show()  # should open in a browser; can also call `plt$write_html(\"<path/to/file.html>\")` plt$to_ordered_dict()$data[[1L]] #> $fill #> [1] \"none\" #>  #> $fillcolor #> [1] \"rgba(68, 68, 68, 0.15)\" #>  #> $line #> $line$color #> [1] \"rgb(31, 119, 180)\" #>  #> $line$shape #> [1] \"hv\" #>  #>  #> $mode #> [1] \"lines\" #>  #> $name #> [1] \"Main\" #>  #> $type #> [1] \"scatter\" #>  #> $x #> [1] 0.0 0.5 1.5 2.5 3.5 5.0 6.5 7.0 #>  #> $xaxis #> [1] \"x\" #>  #> $y #> [1]  0.0003276259  0.0003276259  0.0003276259  0.0003276259 -0.1000587645 #> [6] -0.3973416956 -0.7382006285 -0.7382006285 #>  #> $yaxis #> [1] \"y\""},{"path":"/articles/advanced.html","id":"model-compression-via-the-lasso","dir":"Articles","previous_headings":"","what":"Model compression via the LASSO","title":"Advanced usage","text":"Compared “black-box” models, like random forests deep neural networks, EBMs considered “glass-box” models can competitively accurate also maintaining higher degree transparency explainability. However, EBMs become readily less transparent harder interpret high-dimensional settings many predictor variables; also become difficult use production due increases scoring time. propose simple solution based least absolute shrinkage selection operator (LASSO) can help introduce sparsity reweighting individual model terms removing less relevant ones, thereby allowing models maintain transparency relatively fast scoring times higher-dimensional settings. short, post-processing fitted EBM many (.e., possibly hundreds thousands) terms using LASSO can help reduce model’s complexity drastically improve scoring time. methodological (software) details, see associated paper “Explainable Boosting Machines Sparsity—Maintaining Explainability High-Dimensional Settings.” ’ll illustrate basic idea using previous model, contains 1 total terms comprised main effects, pairwise interactions, intercept. make things easier downstream, lets go ahead partition data separating features response: next chunk, ’ll construct data matrix individual term contributions train test sets. , matrix ’s \\(\\)-th column given \\(\\)-th terms contribution model’s output (applying link function, ’ll logit scale example). particular, note rowSum(terms_trn) + fit$intercept_ equivalent calling predict(fit, newdata = X_trn). data matrices used input LASSO shortly . Next, use glmnet package fit LASSO path new data set comprised individual term contributions. assess performance using associated test set collect results. Next, ’ll compute lambda value associated smallest deviance (half log-loss): results compressed EBM corresponding value lambda: {rebm-ticdata-lasso-results} ebm_lasso_results[.min(ebm_lasso_results$deviance), ] Finally, can plot results! left, show LASSO path right show test deviance function number terms compressed model.  Lastly, can call $scale() $sweep() methods reweight purge unused terms model:  just sanity check, can see compressed (.e., reduced) model produce right predictions. ROC curve via pROC also constructed shown .","code":"# Rebuild the previous EBM without monotonicity fit <- ebm(CARAVAN ~ ., data = tictrn)  fit$term_names_  # does not include the intercept #>   [1] \"STYPE\"               \"MAANTHUI\"            \"MGEMOMV\"             #>   [4] \"MGEMLEEF\"            \"MOSHOOFD\"            \"MGODRK\"              #>   [7] \"MGODPR\"              \"MGODOV\"              \"MGODGE\"              #>  [10] \"MRELGE\"              \"MRELSA\"              \"MRELOV\"              #>  [13] \"MFALLEEN\"            \"MFGEKIND\"            \"MFWEKIND\"            #>  [16] \"MOPLHOOG\"            \"MOPLMIDD\"            \"MOPLLAAG\"            #>  [19] \"MBERHOOG\"            \"MBERZELF\"            \"MBERBOER\"            #>  [22] \"MBERMIDD\"            \"MBERARBG\"            \"MBERARBO\"            #>  [25] \"MSKA\"                \"MSKB1\"               \"MSKB2\"               #>  [28] \"MSKC\"                \"MSKD\"                \"MHHUUR\"              #>  [31] \"MHKOOP\"              \"MAUT1\"               \"MAUT2\"               #>  [34] \"MAUT0\"               \"MZFONDS\"             \"MZPART\"              #>  [37] \"MINKM30\"             \"MINK3045\"            \"MINK4575\"            #>  [40] \"MINK7512\"            \"MINK123M\"            \"MINKGEM\"             #>  [43] \"MKOOPKLA\"            \"PWAPART\"             \"PWABEDR\"             #>  [46] \"PWALAND\"             \"PPERSAUT\"            \"PBESAUT\"             #>  [49] \"PMOTSCO\"             \"AAUT\"                \"PAANHANG\"            #>  [52] \"PTRACTOR\"            \"PWERKT\"              \"PBROM\"               #>  [55] \"PLEVEN\"              \"PPERSONG\"            \"PGEZONG\"             #>  [58] \"PWAOREG\"             \"PBRAND\"              \"PZEILPL\"             #>  [61] \"PPLEZIER\"            \"PFIETS\"              \"PINBOED\"             #>  [64] \"PBYSTAND\"            \"AWAPART\"             \"AWABEDR\"             #>  [67] \"AWALAND\"             \"APERSAUT\"            \"ABESAUT\"             #>  [70] \"AMOTSCO\"             \"AVRAAUT\"             \"AAANHANG\"            #>  [73] \"ATRACTOR\"            \"AWERKT\"              \"ABROM\"               #>  [76] \"ALEVEN\"              \"APERSONG\"            \"AGEZONG\"             #>  [79] \"AWAOREG\"             \"ABRAND\"              \"AZEILPL\"             #>  [82] \"APLEZIER\"            \"AFIETS\"              \"AINBOED\"             #>  [85] \"ABYSTAND\"            \"STYPE & PPERSAUT\"    \"STYPE & PPLEZIER\"    #>  [88] \"STYPE & APERSAUT\"    \"STYPE & APLEZIER\"    \"MAANTHUI & PPERSAUT\" #>  [91] \"MGEMOMV & PPERSAUT\"  \"MGEMLEEF & PPERSAUT\" \"MOSHOOFD & PPERSAUT\" #>  [94] \"MGODRK & PPERSAUT\"   \"MGODPR & PPERSAUT\"   \"MGODOV & PPERSAUT\"   #>  [97] \"MGODGE & PPERSAUT\"   \"MRELGE & PPERSAUT\"   \"MRELGE & PBRAND\"     #> [100] \"MRELSA & PPERSAUT\"   \"MRELOV & PPERSAUT\"   \"MFALLEEN & PPERSAUT\" #> [103] \"MFGEKIND & PPERSAUT\" \"MFWEKIND & PPERSAUT\" \"MOPLHOOG & PPERSAUT\" #> [106] \"MOPLMIDD & PPERSAUT\" \"MOPLLAAG & PPERSAUT\" \"MBERHOOG & PPERSAUT\" #> [109] \"MBERBOER & PPERSAUT\" \"MBERMIDD & PPERSAUT\" \"MBERMIDD & PBRAND\"   #> [112] \"MBERARBG & PPERSAUT\" \"MBERARBG & PBRAND\"   \"MBERARBO & PPERSAUT\" #> [115] \"MSKA & PPERSAUT\"     \"MSKB1 & PPERSAUT\"    \"MSKB1 & PBRAND\"      #> [118] \"MSKB2 & PPERSAUT\"    \"MSKC & PPERSAUT\"     \"MSKD & PPERSAUT\"     #> [121] \"MHHUUR & PPERSAUT\"   \"MHHUUR & PBRAND\"     \"MHKOOP & PPERSAUT\"   #> [124] \"MHKOOP & PBRAND\"     \"MAUT1 & PPERSAUT\"    \"MAUT2 & PPERSAUT\"    #> [127] \"MAUT0 & PPERSAUT\"    \"MZFONDS & PPERSAUT\"  \"MZPART & PPERSAUT\"   #> [130] \"MINKM30 & PPERSAUT\"  \"MINKM30 & PTRACTOR\"  \"MINK3045 & PPERSAUT\" #> [133] \"MINK4575 & PPERSAUT\" \"MINK7512 & PPERSAUT\" \"MINK123M & PPERSAUT\" #> [136] \"MINKGEM & PPERSAUT\"  \"MKOOPKLA & PPERSAUT\" \"MKOOPKLA & PPLEZIER\" #> [139] \"MKOOPKLA & APLEZIER\" \"PWAPART & PPERSAUT\"  \"PWABEDR & PTRACTOR\"  #> [142] \"PWALAND & PPERSAUT\"  \"PPERSAUT & PMOTSCO\"  \"PPERSAUT & PAANHANG\" #> [145] \"PPERSAUT & PTRACTOR\" \"PPERSAUT & PBROM\"    \"PPERSAUT & PLEVEN\"   #> [148] \"PPERSAUT & PGEZONG\"  \"PPERSAUT & PBRAND\"   \"PPERSAUT & PPLEZIER\" #> [151] \"PPERSAUT & PFIETS\"   \"PPERSAUT & PINBOED\"  \"PPERSAUT & PBYSTAND\" #> [154] \"PPERSAUT & AAANHANG\" \"PPERSAUT & ABROM\"    \"PPERSAUT & ALEVEN\"   #> [157] \"PPERSAUT & AGEZONG\"  \"PPERSAUT & ABRAND\"   \"PPERSAUT & APLEZIER\" #> [160] \"PPERSAUT & AFIETS\"   \"PBROM & APERSAUT\"    \"APERSAUT & ABROM\" X_trn <- subset(tictrn, select = -CARAVAN)  # feature columns X_tst <- subset(tictst, select = -CARAVAN) y_trn <- tictrn$CARAVAN y_tst <- tictst$CARAVAN terms_trn <- predict(fit, newdata = X_trn, type = \"terms\") terms_tst <- predict(fit, newdata = X_tst, type = \"terms\") colnames(terms_trn) <- colnames(terms_tst) <- fit$term_names_  # Sanity check head(cbind(   rowSums(terms_trn) + c(fit$intercept_),   predict(fit, newdata = tictrn, type = \"link\")  # additive on link scale )) #>           [,1]      [,2] #> [1,] -2.436647 -2.436647 #> [2,] -3.750533 -3.750533 #> [3,] -2.470428 -2.470428 #> [4,] -2.345187 -2.345187 #> [5,] -6.122288 -6.122288 #> [6,] -5.021766 -5.021766 library(glmnet)  # Fit the LASSO regularization path using the term contributions as inputs ebm_lasso <- glmnet(   x = terms_trn,        # individual term contributions/scores   y = y_trn,            # original response variable   lower.limits = 0,     # coefficients should be strictly positive   standardize = FALSE,  # no need to standardize   family = \"binomial\"   # logistic regression )  # Assess performance of fit using an independent test set perf <- assess.glmnet(   object = ebm_lasso,  # fitted LASSO model   newx = terms_tst,    # test set contributions   newy = y_tst,        # same response variable (test set)   family = \"binomial\"  # for logit link (i.e., logistic regression) ) perf <- do.call(cbind, args = perf)  # bind results into matrix  # Data frame of results (one row for each value of lambda) ebm_lasso_res <- as.data.frame(cbind(   \"num_terms\" = ebm_lasso$df,  # number of non-zero coefficients for each lambda   perf,                        # performance metrics (i.e., deviance, auc, etc.)   \"lambda\" = ebm_lasso$lambda ))  # Sort in ascending order of number of terms head(ebm_lasso_res <- ebm_lasso_res[order(ebm_lasso_res$num_terms), ]) #>    num_terms  deviance      class       auc       mse       mae      lambda #> s0         0 0.4593780 0.06097055 0.5000000 0.1145195 0.2244984 0.010485360 #> s1         1 0.4558854 0.06097055 0.6585024 0.1141333 0.2241294 0.009553870 #> s2         1 0.4530105 0.06097055 0.6585024 0.1138124 0.2237924 0.008705130 #> s3         1 0.4506381 0.06097055 0.6585024 0.1135459 0.2234846 0.007931790 #> s4         1 0.4486776 0.06097055 0.6585024 0.1133243 0.2232036 0.007227152 #> s5         1 0.4470567 0.06097055 0.6585024 0.1131402 0.2229470 0.006585112 (lambda <- ebm_lasso_res[which.min(ebm_lasso_res$deviance), \"lambda\"]) #> [1] 0.0004434513 par(mfrow = c(1, 2), mar = c(4, 4, 0.1, 0.1), cex.lab = 0.95,      cex.axis = 0.8, mgp = c(2, 0.7, 0), tcl = -0.3, las = 1) palette(\"Okabe-Ito\")  # Plot results plot(ebm_lasso, xvar = \"lambda\", col = adjustcolor(3, alpha.f = 0.3)) abline(v = log(lambda), lty = 2, col = 1) plot(ebm_lasso_res[, c(\"num_terms\", \"deviance\")], type = \"l\", las = 1,      xlab = \"Number of terms\", ylab = \"Test deviance\") palette(\"default\") weights <- coef(ebm_lasso, s = lambda) weights <- setNames(as.numeric(weights), nm = rownames(weights)) for (i in seq_along(weights[-1L])) {   idx <- as.integer(i - 1L)  # Sigh, Python indexing starts at 0   print(paste0(\"Replcating term \", fit$term_names_[i], \" with \", names(weights)[i + 1]))   fit$scale(idx, factor = weights[i + 1]) } #> [1] \"Replcating term STYPE with STYPE\" #> [1] \"Replcating term MAANTHUI with MAANTHUI\" #> [1] \"Replcating term MGEMOMV with MGEMOMV\" #> [1] \"Replcating term MGEMLEEF with MGEMLEEF\" #> [1] \"Replcating term MOSHOOFD with MOSHOOFD\" #> [1] \"Replcating term MGODRK with MGODRK\" #> [1] \"Replcating term MGODPR with MGODPR\" #> [1] \"Replcating term MGODOV with MGODOV\" #> [1] \"Replcating term MGODGE with MGODGE\" #> [1] \"Replcating term MRELGE with MRELGE\" #> [1] \"Replcating term MRELSA with MRELSA\" #> [1] \"Replcating term MRELOV with MRELOV\" #> [1] \"Replcating term MFALLEEN with MFALLEEN\" #> [1] \"Replcating term MFGEKIND with MFGEKIND\" #> [1] \"Replcating term MFWEKIND with MFWEKIND\" #> [1] \"Replcating term MOPLHOOG with MOPLHOOG\" #> [1] \"Replcating term MOPLMIDD with MOPLMIDD\" #> [1] \"Replcating term MOPLLAAG with MOPLLAAG\" #> [1] \"Replcating term MBERHOOG with MBERHOOG\" #> [1] \"Replcating term MBERZELF with MBERZELF\" #> [1] \"Replcating term MBERBOER with MBERBOER\" #> [1] \"Replcating term MBERMIDD with MBERMIDD\" #> [1] \"Replcating term MBERARBG with MBERARBG\" #> [1] \"Replcating term MBERARBO with MBERARBO\" #> [1] \"Replcating term MSKA with MSKA\" #> [1] \"Replcating term MSKB1 with MSKB1\" #> [1] \"Replcating term MSKB2 with MSKB2\" #> [1] \"Replcating term MSKC with MSKC\" #> [1] \"Replcating term MSKD with MSKD\" #> [1] \"Replcating term MHHUUR with MHHUUR\" #> [1] \"Replcating term MHKOOP with MHKOOP\" #> [1] \"Replcating term MAUT1 with MAUT1\" #> [1] \"Replcating term MAUT2 with MAUT2\" #> [1] \"Replcating term MAUT0 with MAUT0\" #> [1] \"Replcating term MZFONDS with MZFONDS\" #> [1] \"Replcating term MZPART with MZPART\" #> [1] \"Replcating term MINKM30 with MINKM30\" #> [1] \"Replcating term MINK3045 with MINK3045\" #> [1] \"Replcating term MINK4575 with MINK4575\" #> [1] \"Replcating term MINK7512 with MINK7512\" #> [1] \"Replcating term MINK123M with MINK123M\" #> [1] \"Replcating term MINKGEM with MINKGEM\" #> [1] \"Replcating term MKOOPKLA with MKOOPKLA\" #> [1] \"Replcating term PWAPART with PWAPART\" #> [1] \"Replcating term PWABEDR with PWABEDR\" #> [1] \"Replcating term PWALAND with PWALAND\" #> [1] \"Replcating term PPERSAUT with PPERSAUT\" #> [1] \"Replcating term PBESAUT with PBESAUT\" #> [1] \"Replcating term PMOTSCO with PMOTSCO\" #> [1] \"Replcating term AAUT with AAUT\" #> [1] \"Replcating term PAANHANG with PAANHANG\" #> [1] \"Replcating term PTRACTOR with PTRACTOR\" #> [1] \"Replcating term PWERKT with PWERKT\" #> [1] \"Replcating term PBROM with PBROM\" #> [1] \"Replcating term PLEVEN with PLEVEN\" #> [1] \"Replcating term PPERSONG with PPERSONG\" #> [1] \"Replcating term PGEZONG with PGEZONG\" #> [1] \"Replcating term PWAOREG with PWAOREG\" #> [1] \"Replcating term PBRAND with PBRAND\" #> [1] \"Replcating term PZEILPL with PZEILPL\" #> [1] \"Replcating term PPLEZIER with PPLEZIER\" #> [1] \"Replcating term PFIETS with PFIETS\" #> [1] \"Replcating term PINBOED with PINBOED\" #> [1] \"Replcating term PBYSTAND with PBYSTAND\" #> [1] \"Replcating term AWAPART with AWAPART\" #> [1] \"Replcating term AWABEDR with AWABEDR\" #> [1] \"Replcating term AWALAND with AWALAND\" #> [1] \"Replcating term APERSAUT with APERSAUT\" #> [1] \"Replcating term ABESAUT with ABESAUT\" #> [1] \"Replcating term AMOTSCO with AMOTSCO\" #> [1] \"Replcating term AVRAAUT with AVRAAUT\" #> [1] \"Replcating term AAANHANG with AAANHANG\" #> [1] \"Replcating term ATRACTOR with ATRACTOR\" #> [1] \"Replcating term AWERKT with AWERKT\" #> [1] \"Replcating term ABROM with ABROM\" #> [1] \"Replcating term ALEVEN with ALEVEN\" #> [1] \"Replcating term APERSONG with APERSONG\" #> [1] \"Replcating term AGEZONG with AGEZONG\" #> [1] \"Replcating term AWAOREG with AWAOREG\" #> [1] \"Replcating term ABRAND with ABRAND\" #> [1] \"Replcating term AZEILPL with AZEILPL\" #> [1] \"Replcating term APLEZIER with APLEZIER\" #> [1] \"Replcating term AFIETS with AFIETS\" #> [1] \"Replcating term AINBOED with AINBOED\" #> [1] \"Replcating term ABYSTAND with ABYSTAND\" #> [1] \"Replcating term STYPE & PPERSAUT with STYPE & PPERSAUT\" #> [1] \"Replcating term STYPE & PPLEZIER with STYPE & PPLEZIER\" #> [1] \"Replcating term STYPE & APERSAUT with STYPE & APERSAUT\" #> [1] \"Replcating term STYPE & APLEZIER with STYPE & APLEZIER\" #> [1] \"Replcating term MAANTHUI & PPERSAUT with MAANTHUI & PPERSAUT\" #> [1] \"Replcating term MGEMOMV & PPERSAUT with MGEMOMV & PPERSAUT\" #> [1] \"Replcating term MGEMLEEF & PPERSAUT with MGEMLEEF & PPERSAUT\" #> [1] \"Replcating term MOSHOOFD & PPERSAUT with MOSHOOFD & PPERSAUT\" #> [1] \"Replcating term MGODRK & PPERSAUT with MGODRK & PPERSAUT\" #> [1] \"Replcating term MGODPR & PPERSAUT with MGODPR & PPERSAUT\" #> [1] \"Replcating term MGODOV & PPERSAUT with MGODOV & PPERSAUT\" #> [1] \"Replcating term MGODGE & PPERSAUT with MGODGE & PPERSAUT\" #> [1] \"Replcating term MRELGE & PPERSAUT with MRELGE & PPERSAUT\" #> [1] \"Replcating term MRELGE & PBRAND with MRELGE & PBRAND\" #> [1] \"Replcating term MRELSA & PPERSAUT with MRELSA & PPERSAUT\" #> [1] \"Replcating term MRELOV & PPERSAUT with MRELOV & PPERSAUT\" #> [1] \"Replcating term MFALLEEN & PPERSAUT with MFALLEEN & PPERSAUT\" #> [1] \"Replcating term MFGEKIND & PPERSAUT with MFGEKIND & PPERSAUT\" #> [1] \"Replcating term MFWEKIND & PPERSAUT with MFWEKIND & PPERSAUT\" #> [1] \"Replcating term MOPLHOOG & PPERSAUT with MOPLHOOG & PPERSAUT\" #> [1] \"Replcating term MOPLMIDD & PPERSAUT with MOPLMIDD & PPERSAUT\" #> [1] \"Replcating term MOPLLAAG & PPERSAUT with MOPLLAAG & PPERSAUT\" #> [1] \"Replcating term MBERHOOG & PPERSAUT with MBERHOOG & PPERSAUT\" #> [1] \"Replcating term MBERBOER & PPERSAUT with MBERBOER & PPERSAUT\" #> [1] \"Replcating term MBERMIDD & PPERSAUT with MBERMIDD & PPERSAUT\" #> [1] \"Replcating term MBERMIDD & PBRAND with MBERMIDD & PBRAND\" #> [1] \"Replcating term MBERARBG & PPERSAUT with MBERARBG & PPERSAUT\" #> [1] \"Replcating term MBERARBG & PBRAND with MBERARBG & PBRAND\" #> [1] \"Replcating term MBERARBO & PPERSAUT with MBERARBO & PPERSAUT\" #> [1] \"Replcating term MSKA & PPERSAUT with MSKA & PPERSAUT\" #> [1] \"Replcating term MSKB1 & PPERSAUT with MSKB1 & PPERSAUT\" #> [1] \"Replcating term MSKB1 & PBRAND with MSKB1 & PBRAND\" #> [1] \"Replcating term MSKB2 & PPERSAUT with MSKB2 & PPERSAUT\" #> [1] \"Replcating term MSKC & PPERSAUT with MSKC & PPERSAUT\" #> [1] \"Replcating term MSKD & PPERSAUT with MSKD & PPERSAUT\" #> [1] \"Replcating term MHHUUR & PPERSAUT with MHHUUR & PPERSAUT\" #> [1] \"Replcating term MHHUUR & PBRAND with MHHUUR & PBRAND\" #> [1] \"Replcating term MHKOOP & PPERSAUT with MHKOOP & PPERSAUT\" #> [1] \"Replcating term MHKOOP & PBRAND with MHKOOP & PBRAND\" #> [1] \"Replcating term MAUT1 & PPERSAUT with MAUT1 & PPERSAUT\" #> [1] \"Replcating term MAUT2 & PPERSAUT with MAUT2 & PPERSAUT\" #> [1] \"Replcating term MAUT0 & PPERSAUT with MAUT0 & PPERSAUT\" #> [1] \"Replcating term MZFONDS & PPERSAUT with MZFONDS & PPERSAUT\" #> [1] \"Replcating term MZPART & PPERSAUT with MZPART & PPERSAUT\" #> [1] \"Replcating term MINKM30 & PPERSAUT with MINKM30 & PPERSAUT\" #> [1] \"Replcating term MINKM30 & PTRACTOR with MINKM30 & PTRACTOR\" #> [1] \"Replcating term MINK3045 & PPERSAUT with MINK3045 & PPERSAUT\" #> [1] \"Replcating term MINK4575 & PPERSAUT with MINK4575 & PPERSAUT\" #> [1] \"Replcating term MINK7512 & PPERSAUT with MINK7512 & PPERSAUT\" #> [1] \"Replcating term MINK123M & PPERSAUT with MINK123M & PPERSAUT\" #> [1] \"Replcating term MINKGEM & PPERSAUT with MINKGEM & PPERSAUT\" #> [1] \"Replcating term MKOOPKLA & PPERSAUT with MKOOPKLA & PPERSAUT\" #> [1] \"Replcating term MKOOPKLA & PPLEZIER with MKOOPKLA & PPLEZIER\" #> [1] \"Replcating term MKOOPKLA & APLEZIER with MKOOPKLA & APLEZIER\" #> [1] \"Replcating term PWAPART & PPERSAUT with PWAPART & PPERSAUT\" #> [1] \"Replcating term PWABEDR & PTRACTOR with PWABEDR & PTRACTOR\" #> [1] \"Replcating term PWALAND & PPERSAUT with PWALAND & PPERSAUT\" #> [1] \"Replcating term PPERSAUT & PMOTSCO with PPERSAUT & PMOTSCO\" #> [1] \"Replcating term PPERSAUT & PAANHANG with PPERSAUT & PAANHANG\" #> [1] \"Replcating term PPERSAUT & PTRACTOR with PPERSAUT & PTRACTOR\" #> [1] \"Replcating term PPERSAUT & PBROM with PPERSAUT & PBROM\" #> [1] \"Replcating term PPERSAUT & PLEVEN with PPERSAUT & PLEVEN\" #> [1] \"Replcating term PPERSAUT & PGEZONG with PPERSAUT & PGEZONG\" #> [1] \"Replcating term PPERSAUT & PBRAND with PPERSAUT & PBRAND\" #> [1] \"Replcating term PPERSAUT & PPLEZIER with PPERSAUT & PPLEZIER\" #> [1] \"Replcating term PPERSAUT & PFIETS with PPERSAUT & PFIETS\" #> [1] \"Replcating term PPERSAUT & PINBOED with PPERSAUT & PINBOED\" #> [1] \"Replcating term PPERSAUT & PBYSTAND with PPERSAUT & PBYSTAND\" #> [1] \"Replcating term PPERSAUT & AAANHANG with PPERSAUT & AAANHANG\" #> [1] \"Replcating term PPERSAUT & ABROM with PPERSAUT & ABROM\" #> [1] \"Replcating term PPERSAUT & ALEVEN with PPERSAUT & ALEVEN\" #> [1] \"Replcating term PPERSAUT & AGEZONG with PPERSAUT & AGEZONG\" #> [1] \"Replcating term PPERSAUT & ABRAND with PPERSAUT & ABRAND\" #> [1] \"Replcating term PPERSAUT & APLEZIER with PPERSAUT & APLEZIER\" #> [1] \"Replcating term PPERSAUT & AFIETS with PPERSAUT & AFIETS\" #> [1] \"Replcating term PBROM & APERSAUT with PBROM & APERSAUT\" #> [1] \"Replcating term APERSAUT & ABROM with APERSAUT & ABROM\" fit$intercept_ <- weights[1L]  # Remove unused terms! fit$sweep(terms = TRUE, bins = TRUE, features = FALSE) #> ExplainableBoostingClassifier(early_stopping_tolerance=0, #>                               interaction_smoothing_rounds=100, #>                               learning_rate=0.04, max_leaves=2, min_hessian=0.0, #>                               smoothing_rounds=500) length(fit$term_names_) #> [1] 27 plot(fit)  # show new feature importance scores # Compare predictions to LASSO fit head(p <- predict(fit, newdata = X_tst)) #>           [,1]        [,2] #> [1,] 0.9790247 0.020975310 #> [2,] 0.9819993 0.018000657 #> [3,] 0.9907643 0.009235741 #> [4,] 0.9098090 0.090190968 #> [5,] 0.9531169 0.046883138 #> [6,] 0.9443282 0.055671789 head(predict(ebm_lasso, terms_tst, s = lambda, type = \"response\")) #>               s1 #> [1,] 0.020975310 #> [2,] 0.018000657 #> [3,] 0.009235741 #> [4,] 0.090190968 #> [5,] 0.046883138 #> [6,] 0.055671789  # ROC curve and AUROC statistic (roc <- pROC::roc(response = y_tst, predictor = p[, 2L])) #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases #>  #> Call: #> roc.default(response = y_tst, predictor = p[, 2L]) #>  #> Data: p[, 2] in 4528 controls (y_tst 0) < 294 cases (y_tst 1). #> Area under the curve: 0.735 plot(roc)"},{"path":"/articles/ebm.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting started","title":"intro","text":"package requires reticulate appropriate Python environment interpret library installed. ’re Python user, ’d suggest reading reticulate vignettes learn . particular: See vignette(\"versions\", package = \"reticulate\") learning Python version configuration. See vignette(\"python_packages\", package = \"reticulate\") learning installing Python packages. See vignette(\"python_primer\", package = \"reticulate\") learning interacting Python objects reticulate (can useful understanding advanced usage ebm package) simplicity, package also provides install_interpret() function installing sole Python library dependency; see ebm::?install_interpret() details. following section provides brief overview ebm() function. detailed usage examples, read following articles: producing interactive plots, see vignette(\"interactive\"). merging several fitted EBM models one, see vignette(\"merging\"). advanced usage package, well classification example, see vignette(\"advanced\").","code":""},{"path":"/articles/ebm.html","id":"using-the-ebm-package","dir":"Articles","previous_headings":"","what":"Using the ebm package","title":"intro","text":"illustrate regression case, ’ll use Hitters data ISLR2 package. sample data shown . ’ll start fitting basic EBMRegressor hitters data set using Salary response. Note ebm() function currently supports usual R formula interface. can obtain predictions using familiar predict() method employed modeling packages R. Note bagging, EBMs can provide standard errors predictions requested. can produce several plotly-based graphs help interpret output \"EBM\" objects using generic plot() method; function supports global local interpretations. default simply prints global measure importance based sum absolute value variable’s term contributions. (Markdown-type documents, like vignette, need specify display = \"markdown\"; see ?ebm::plot details.)  can also plot individual shape functions (term contributions), shown :  Pairwise interactions can visualized supplying plot() appropriate pair variables.  can also display local explanations (though, one time) specifying local = TRUE:","code":"data(\"Hitters\", package = \"ISLR2\")  # Remove rows with missing response values head(hitters <- Hitters[!is.na(Hitters$Salary), ]) #>                   AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun #> -Alan Ashby         315   81     7   24  38    39    14   3449   835     69 #> -Alvin Davis        479  130    18   66  72    76     3   1624   457     63 #> -Andre Dawson       496  141    20   65  78    37    11   5628  1575    225 #> -Andres Galarraga   321   87    10   39  42    30     2    396   101     12 #> -Alfredo Griffin    594  169     4   74  51    35    11   4408  1133     19 #> -Al Newman          185   37     1   23   8    21     2    214    42      1 #>                   CRuns CRBI CWalks League Division PutOuts Assists Errors #> -Alan Ashby         321  414    375      N        W     632      43     10 #> -Alvin Davis        224  266    263      A        W     880      82     14 #> -Andre Dawson       828  838    354      N        E     200      11      3 #> -Andres Galarraga    48   46     33      N        E     805      40      4 #> -Alfredo Griffin    501  336    194      A        W     282     421     25 #> -Al Newman           30    9     24      N        E      76     127      7 #>                   Salary NewLeague #> -Alan Ashby        475.0         N #> -Alvin Davis       480.0         A #> -Andre Dawson      500.0         N #> -Andres Galarraga   91.5         N #> -Alfredo Griffin   750.0         A #> -Al Newman          70.0         A library(ebm)  # Fit a default EBM regressor fit <- ebm(Salary ~ ., data = hitters, objective = \"rmse\") fit  # still need to implement print() and summary() methods #> ExplainableBoostingRegressor(early_stopping_tolerance=0) head(predict(fit, newdata = hitters)) #> [1] 489.4548 626.1970 870.3430 169.8797 659.6543 270.5382  # Ask for predictions and standard errors head(predict(fit, newdata = hitters, se_fit = TRUE)) #>          [,1]      [,2] #> [1,] 489.4548  53.92471 #> [2,] 626.1970  98.39167 #> [3,] 870.3430 241.04199 #> [4,] 169.8797  43.14385 #> [5,] 659.6543  54.02381 #> [6,] 270.5382 155.71903 library(ggplot2)  theme_set(theme_bw())  # Plot feature importance (i.e., mean absolute scores) plot(fit, n_features = 15) plot(fit, term = \"Years\") plot(fit, term = c(\"Hits\", \"CAtBat\")) # Understand an individual prediction x <- subset(hitters, select = -Salary)[1L, ]  # use first observation plot(fit, local = TRUE, X = x, y = hitters$Salary[1L], geom = \"col\")"},{"path":"/articles/longintro.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting started","title":"ebm","text":"TBD. Talk setting reticulate, package installation via install_interpret(), etc.","code":""},{"path":"/articles/longintro.html","id":"using-the-ebm-package","dir":"Articles","previous_headings":"","what":"Using the ebm package","title":"ebm","text":"illustrate regression case, ’ll use Hitters data ISLR2 package. sample data shown . ’ll start fitting basic EBMRegressor hitters data set using Salary response. Note ebm() function currently supports usual R formula interface. can obtain predictions using familiar predict() method employed modeling packages R. Note bagging, EBMs can provide standard errors predictions requested. can produce several plotly-based graphs help interpret output \"EBM\" objects using generic plot() method; function supports global local interpretations. default simply prints global measure importance based sum absolute value variable sterm contributions. (Markdown-type documents, like vignette, need specify display = \"markdown\"; see ?ebm::plot details.)  can also plot individual shape functions (term contributions), shown : Plots can also optionally interactive via HTML setting interactive = TRUE: ebm package expose 100% functionality available Python, can pretty much anything need interacting directly underlying Python objects (magic happens reticulate). instance, invoke monotonize() method fit post process term \"Years\" enforcing increasing monotonicity:   can also display local explanations (though, one time) specifying local = TRUE:","code":"data(\"Hitters\", package = \"ISLR2\")  # Remove rows with missing response values head(hitters <- Hitters[!is.na(Hitters$Salary), ]) #>                   AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun #> -Alan Ashby         315   81     7   24  38    39    14   3449   835     69 #> -Alvin Davis        479  130    18   66  72    76     3   1624   457     63 #> -Andre Dawson       496  141    20   65  78    37    11   5628  1575    225 #> -Andres Galarraga   321   87    10   39  42    30     2    396   101     12 #> -Alfredo Griffin    594  169     4   74  51    35    11   4408  1133     19 #> -Al Newman          185   37     1   23   8    21     2    214    42      1 #>                   CRuns CRBI CWalks League Division PutOuts Assists Errors #> -Alan Ashby         321  414    375      N        W     632      43     10 #> -Alvin Davis        224  266    263      A        W     880      82     14 #> -Andre Dawson       828  838    354      N        E     200      11      3 #> -Andres Galarraga    48   46     33      N        E     805      40      4 #> -Alfredo Griffin    501  336    194      A        W     282     421     25 #> -Al Newman           30    9     24      N        E      76     127      7 #>                   Salary NewLeague #> -Alan Ashby        475.0         N #> -Alvin Davis       480.0         A #> -Andre Dawson      500.0         N #> -Andres Galarraga   91.5         N #> -Alfredo Griffin   750.0         A #> -Al Newman          70.0         A library(ebm)  # Fit a default EBM regressor fit <- ebm(Salary ~ ., data = hitters, objective = \"rmse\") fit  # still need to implement print() and summary() methods #> ExplainableBoostingRegressor(early_stopping_tolerance=0) head(predict(fit, newdata = hitters)) #> [1] 489.4548 626.1970 870.3430 169.8797 659.6543 270.5382  # Ask for predictions and standard errors head(predict(fit, newdata = hitters, se.fit = TRUE)) #> [1] 489.4548 626.1970 870.3430 169.8797 659.6543 270.5382 library(ggplot2)  theme_set(theme_bw())  # Plot feature importance (i.e., mean absolute scores) plot(fit) plot(fit, term = \"Years\") plot(fit, term = \"Years\", interactive = TRUE, display = \"markdown\") fit$monotonize(\"Years\", increasing = FALSE) #> ExplainableBoostingRegressor(early_stopping_tolerance=0) plot(fit, term = \"Years\") plot(fit, term = c(\"Hits\", \"CAtBat\")) # Understand an individual prediction x <- subset(hitters, select = -Salary)[1L, ]  # use first observation plot(fit, local = TRUE, X = x, y = hitters$Salary[1L], geom = \"col\") # Understand an individual prediction plot(fit, local = TRUE, X = x, y = hitters$Salary[1L], display = \"markdown\")"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Brandon Greenwell. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Greenwell B (2025). ebm: Package (Title Case). R package version 0.1.0.","code":"@Manual{,   title = {ebm: What the Package Does (Title Case)},   author = {Brandon Greenwell},   year = {2025},   note = {R package version 0.1.0}, }"},{"path":"/index.html","id":"ebm","dir":"","previous_headings":"","what":"What the Package Does (Title Case)","title":"What the Package Does (Title Case)","text":"simple R wrapper around explainable boosting functionality interpret library Python. Currently, can install ebm package GitHub:","code":"# install.packages(\"remotes\") remotes::install_github(\"bgreenwell/ebm\")"},{"path":"/reference/ebm-package.html","id":null,"dir":"Reference","previous_headings":"","what":"ebm: What the Package Does (Title Case) — ebm-package","title":"ebm: What the Package Does (Title Case) — ebm-package","text":"(maybe one line). Continuation lines indented.","code":""},{"path":"/reference/ebm-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"ebm: What the Package Does (Title Case) — ebm-package","text":"Maintainer: Brandon Greenwell greenwell.brandon@gmail.com (ORCID)","code":""},{"path":"/reference/ebm.html","id":null,"dir":"Reference","previous_headings":"","what":"Explainable Boosting Machine (EBM) — ebm","title":"Explainable Boosting Machine (EBM) — ebm","text":"function R wrapper explainable boosting functions Python interpret library. trains Explainable Boosting Machine (EBM) model, tree-based, cyclic gradient boosting generalized additive model automatic interaction detection. EBMs often accurate state---art blackbox models remaining completely interpretable.","code":""},{"path":"/reference/ebm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Explainable Boosting Machine (EBM) — ebm","text":"","code":"ebm(   formula,   data,   max_bins = 1024L,   max_interaction_bins = 64L,   interactions = 0.9,   exclude = NULL,   validation_size = 0.15,   outer_bags = 16L,   inner_bags = 0L,   learning_rate = 0.04,   greedy_ratio = 10,   cyclic_progress = FALSE,   smoothing_rounds = 500L,   interaction_smoothing_rounds = 100L,   max_rounds = 25000L,   early_stopping_rounds = 100L,   early_stopping_tolerance = 1e-05,   min_samples_leaf = 4L,   min_hessian = 0,   reg_alpha = 0,   reg_lambda = 0,   max_delta_step = 0,   gain_scale = 5,   min_cat_samples = 10L,   cat_smooth = 10,   missing = \"separate\",   max_leaves = 2L,   monotone_constraints = NULL,   objective = c(\"log_loss\", \"rmse\", \"poisson_deviance\",     \"tweedie_deviance:variance_power=1.5\", \"gamma_deviance\", \"pseudo_huber:delta=1.0\",     \"rmse_log\"),   n_jobs = -1L,   random_state = 42L,   ... )"},{"path":"/reference/ebm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Explainable Boosting Machine (EBM) — ebm","text":"formula formula form y ~ x1 + x2 + .... data data frame containing variables model. max_bins Max number bins per feature main effects stage. Default 1024. max_interaction_bins Max number bins per feature interaction terms. Default 64. interactions Interaction terms included model. Default 0.9. Options : Integer (1 <= interactions): Count interactions automatically selected Percentage (interactions < 1.0): Determine integer count interactions multiplying number features percentage List numeric pairs: pairs contain indices features within additive term. addition pairs, interactions parameter accepts higher order interactions. also accepts univariate terms cause algorithm boost main terms time interactions. boosting mains time interactions, exclude parameter set \"mains\" currently max_bins needs equal max_interaction_bins. exclude Features terms excluded. Default NULL. validation_size Validation set size. Used early stopping boosting, needed create outer bags. Default 0.15. Options : Integer (1 <= validation_size): Count samples put validation sets. Percentage (validation_size < 1.0): Percentage data put validation sets. 0: Turns early stopping. Outer bags utility. Error bounds eliminated. outer_bags Number outer bags. Outer bags used generate error bounds help smoothing graphs. Default 16. inner_bags Number inner bags. Default 0. learning_rate Learning rate boosting. Default 0.04. greedy_ratio proportion greedy boosting steps relative cyclic boosting steps. value 0 disables greedy boosting, effectively turning . Default 10.0. cyclic_progress parameter specifies proportion boosting cycles actively contribute improving model’s performance. expressed logical numeric 0 1, default set TRUE (1.0), meaning 100% cycles expected make forward progress. forward progress achieved cycle, cycle wasted; instead, used update internal gain calculations related effective feature predicting target variable. Setting parameter value less 1.0 can useful preventing overfitting. Default FALSE. smoothing_rounds Number initial highly regularized rounds set basic shape main effect feature graphs. Default 500. interaction_smoothing_rounds Number initial highly regularized rounds set basic shape interaction effect feature graphs fitting. Default 100. max_rounds Total number boosting rounds n_terms boosting steps per round. Default 25000. early_stopping_rounds Number rounds improvement trigger early stopping. 0 turns early stopping boosting occur exactly max_rounds. Default 100. early_stopping_tolerance Tolerance dictates smallest delta required considered improvement prevents algorithm early stopping. early_stopping_tolerance expressed percentage early stopping metric. Negative values indicate individual models overfit stopping. EBMs bagged ensemble models. Setting early_stopping_tolerance zero (even negative), allows learning overfit individual models little, can improve accuracy ensemble whole. Overfitting individual models reduces bias model expense increasing variance (due overfitting) individual models. averaging models ensemble reduces variance without much change bias. Since goal find optimum bias-variance tradeoff ensemble models---individual models---small amount overfitting individual models can improve accuracy ensemble whole. Default 1e-05. min_samples_leaf Minimum number samples allowed leaves. Default 4. min_hessian Minimum hessian required consider potential split valid. Default 0.0. reg_alpha L1 regularization. Default 0.0. reg_lambda L2 regularization. Default 0.0. max_delta_step Used limit max output tree leaves. <=0.0 means constraint. Default 0.0. gain_scale Scale factor apply nominal categoricals. scale factor 1.0 cause algorithm focus nominal categoricals. Default 5.0. min_cat_samples Minimum number samples order treat category separately. lower threshold category combined categories low numbers samples. Default 10. cat_smooth Used categorical features. can reduce effect noises categorical features, especially categories limited data. Default 10.0. missing Method handling missing values boosting. Default \"separate\". placement missing value bin can influence resulting model graphs. example, placing bin “low” side may cause missing values affect lower bins, vice versa. parameter affect final placement missing bin model (missing bin remain index 0 term_scores_ attribute). Possible values missing : \"low\": Place missing bin left side graphs. \"high\": Place missing bin right side graphs. \"separate\": Place missing bin leaf boosting step, effectively making location-agnostic. can lead overfitting, especially proportion missing values small. \"gain\": Choose best leaf missing value contribution boosting step, based gain. max_leaves Maximum number leaves allowed tree. Default 2. monotone_constraints Default NULL. parameter allows specify monotonic constraints feature's relationship target variable model fitting. However, generally recommended apply monotonic constraints post-fit using monotonize() attribute rather setting fitting process. recommendation based observation , fitting, boosting algorithm may compensate monotone constraint one feature utilizing another correlated feature, potentially obscuring monotonic violations. choose define monotone constraints, monotone_constraints numeric vector length equal number features. element list corresponds feature take one following values: 0: monotonic constraint imposed corresponding feature’s partial response. +1: partial response corresponding feature monotonically increasing respect target. -1: partial response corresponding feature monotonically decreasing respect target. objective objective function optimize. Default \"log_loss\" classification \"rmse\" regression. n_jobs Number jobs run parallel. Default -1. Negative integers interpreted following joblib's formula (n_cpus + 1 + n_jobs), just like scikit-learn. example, n_jobs = -2 means using threads except 1. random_state Random state. Setting NULL generates non-repeatable sequences. ... Additional optional argument. (Currently ignored.)","code":""},{"path":"/reference/ebm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Explainable Boosting Machine (EBM) — ebm","text":"trained EBM model.","code":""},{"path":"/reference/ebm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Explainable Boosting Machine (EBM) — ebm","text":"","code":"if (FALSE) {   data(\"Hitters\", package = \"ISLR2\")    # Remove rows with missing response values   hitters <- Hitters[!is.na(Hitters$Salary), ]    # Fit a default EBM regressor   fit <- ebm(Salary ~ ., data = hitters, objective = \"rmse\")    # Generate some predictions   head(predict(fit, newdata = hitters))   head(predict(fit, newdata = hitters, se.fit = TRUE))    # Show global summary and GAM shape functions   plot(fit)   plot(fit, term = \"Years\")   plot(fit, display = \"url\")  # can paste the resulting URL in the browser    # Understand an individual prediction   x <- subset(hitters, select = -Salary)[1L, ]  # use first observation   ebm_show(fit, local = TRUE, X = x, y = hitters$Salary[1L]) }"},{"path":"/reference/geom_stepribbon.html","id":null,"dir":"Reference","previous_headings":"","what":"Step ribbons and area plots — geom_stepribbon","title":"Step ribbons and area plots — geom_stepribbon","text":"combination geom_ribbon() geom_step().","code":""},{"path":"/reference/geom_stepribbon.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Step ribbons and area plots — geom_stepribbon","text":"","code":"geom_stepribbon(   mapping = NULL,   data = NULL,   stat = \"identity\",   position = \"identity\",   na.rm = FALSE,   show.legend = NA,   inherit.aes = TRUE,   ... )"},{"path":"/reference/geom_stepribbon.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Step ribbons and area plots — geom_stepribbon","text":"Taken ldatools.","code":""},{"path":"/reference/geom_stepribbon.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Step ribbons and area plots — geom_stepribbon","text":"mapping Set aesthetic mappings created aes(). specified inherit.aes = TRUE (default), combined default mapping top level plot. must supply mapping plot mapping. data data displayed layer. three options: NULL, default, data inherited plot data specified call ggplot(). data.frame, object, override plot data. objects fortified produce data frame. See fortify() variables created. function called single argument, plot data. return value must data.frame, used layer data. function can created formula (e.g. ~ head(.x, 10)). stat statistical transformation use data layer, either ggproto Geom subclass string naming stat stripped stat_ prefix (e.g. \"count\" rather \"stat_count\") position Position adjustment, either string naming adjustment (e.g. \"jitter\" use position_jitter), result call position adjustment function. Use latter need change settings adjustment. na.rm FALSE, default, missing values removed warning. TRUE, missing values silently removed. show.legend logical. layer included legends? NA, default, includes aesthetics mapped. FALSE never includes, TRUE always includes. can also named logical vector finely select aesthetics display. inherit.aes FALSE, overrides default aesthetics, rather combining . useful helper functions define data aesthetics inherit behaviour default plot specification, e.g. borders(). ... arguments passed layer(). often aesthetics, used set aesthetic fixed value, like colour = \"red\" size = 3. may also parameters paired geom/stat.","code":""},{"path":"/reference/install_interpret.html","id":null,"dir":"Reference","previous_headings":"","what":"Install interpret — install_interpret","title":"Install interpret — install_interpret","text":"function install interpret along dependencies.","code":""},{"path":"/reference/install_interpret.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install interpret — install_interpret","text":"","code":"install_interpret(   envname = \"r-ebm\",   ...,   extra_packages = c(\"plotly>=3.8.1\"),   python_version = \">=3.9,<=3.12\",   restart_session = TRUE )"},{"path":"/reference/install_interpret.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Install interpret — install_interpret","text":"envname Name path Python virtual environment. ... Additional optional arguments. (Currently ignored.) extra_packages Additional Python packages install alongside interpret. python_version Passed reticulate::virtualenv_starter() restart_session Whether restart R session installing (note occur within RStudio).","code":""},{"path":"/reference/install_interpret.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Install interpret — install_interpret","text":"return value, called side effects.","code":""},{"path":"/reference/merge.EBM.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge method for EBM objects — merge.EBM","title":"Merge method for EBM objects — merge.EBM","text":"Merge multiple EBMs together.","code":""},{"path":"/reference/merge.EBM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge method for EBM objects — merge.EBM","text":"","code":"# S3 method for EBM merge(x, y, ...)"},{"path":"/reference/merge.EBM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge method for EBM objects — merge.EBM","text":"x, y Fitted ebm objects (.e., objects class \"EBM\") trained similar data sets set features. ... Additional ebm objects merged.","code":""},{"path":"/reference/merge.EBM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge method for EBM objects — merge.EBM","text":"merged ebm object.","code":""},{"path":"/reference/plot.EBM.html","id":null,"dir":"Reference","previous_headings":"","what":"Interpret plots for fitted EBM objects — plot.EBM","title":"Interpret plots for fitted EBM objects — plot.EBM","text":"Provides interactive visualization given explanation(s).","code":""},{"path":"/reference/plot.EBM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interpret plots for fitted EBM objects — plot.EBM","text":"","code":"# S3 method for EBM plot(   x,   term = NULL,   local = FALSE,   X = NULL,   y = NULL,   init_score = NULL,   interactive = FALSE,   n_features = NULL,   geom = c(\"point\", \"col\"),   mapping = NULL,   aesthetics = list(),   horizontal = FALSE,   uncertainty = TRUE,   width = 0.5,   alpha = 0.5,   fill = \"grey\",   display = c(\"viewer\", \"markdown\", \"url\"),   viewer = c(\"browser\", \"rstudio\"),   full_dashboard = FALSE,   ... )"},{"path":"/reference/plot.EBM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interpret plots for fitted EBM objects — plot.EBM","text":"x EBMClassifier EBMRegressor object. term Character string specifying term plot. interaction effect, can supply pair (e.g., term = c(\"x1\", \"x2\")). Default NULL just display overall importance term. local Logocial indicating whether display local explanations (TRUE) global explanations (FALSE). Default FALSE. X Data frame matrix samples. Unless display = \"url\" full_dashboard = TRUE, X can contain single row. y Optional vector response values corresponding X. init_score Optional. Either model can generate scores per-sample initialization score. samples scores length X. interactive Logical indicating whether produce interactive plot based HTML. Default FALSE. (type = \"static\") interactive plot (type = \"plotly\") n_features Integer specifying maximum number variable importance scores plot. Default NULL corresponds features. geom Character string specifying type plot construct terms associated categorical features. Current options : geom = \"col\" uses geom_col construct bar chart scores. geom = \"point\" uses geom_point construct Cleveland dot plot term scores. Default \"point\". mapping Set aesthetic mappings created aes-related functions /tidy eval helpers. See example usage . aesthetics List specifying additional arguments passed layer. often aesthetics, used set aesthetic fixed value, likecolour = \"red\" size = 3. See example usage . horizontal Logical indicating whether term plots categorical features flipped horzintally. Default FALSE. uncertainty Logical indicating whether also display uncertainty via error bars main effect plots. Default TRUE. useful unless outer_bags > 1 calling ebm(). width Numeric specifying width error bars displayed bar/ dot plots categorical features. Default 0.5. alpha Numeric 0 1 specifying level transparency use displaying uncertainty plots continuous features. Default 0.5. fill Character string specifying fill color use displaying uncertainty plots continuous features. Default \"grey\". display Character string specifying results displayed whenever interactive = TRUE. Available options \"viewer\" (e.g., RStudio viewer browser), \"markdown\" (e.g., vingettes, Quarto, Rmarkdown documents), \"url\" (e.g., print URL can pasted browser). display = \"url\", URL viewing entire interpret dashboard provided (.e., term full_dashboard arguments ignored). viewer Character string specifying results viewed. Current choices \"broswer\", calls utils::browseURL() display results HTML browser, \"rstudio\" displaying results within Viewer pane active RStudio session. Also works VS Code. Default \"browser\". full_dashboard Logical indicating whether display full interpret dashboard. Default FALSE. works display = \"viewer\" display = \"url\" (e.g., paste resulting URL browser). ... Additional optional arguments. Currently passed onto levelplot() heatmaps interaction effects.","code":""},{"path":"/reference/predict.EBM.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict method for EBM objects — predict.EBM","title":"Predict method for EBM objects — predict.EBM","text":"Compute predicted values fitted explainable boosting machine.","code":""},{"path":"/reference/predict.EBM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict method for EBM objects — predict.EBM","text":"","code":"# S3 method for EBM predict(   object,   newdata,   type = c(\"response\", \"link\", \"class\", \"terms\"),   se_fit = FALSE,   init_score = NULL,   ... )"},{"path":"/reference/predict.EBM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict method for EBM objects — predict.EBM","text":"object fitted ebm object. newdata data frame look variables predict. type type prediction required. se_fit Logical indicating whether standard errors required. init_score Optional. Either model can generate scores per-sample initialization score. samples scores length newdata. ... Additional optional arguments. (Currently ignored.)","code":""},{"path":"/reference/predict.EBM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict method for EBM objects — predict.EBM","text":"regression (.e., EBMReegressor objects), either vector predictions (se_fit = FALSE) two-column matrix (se = TRUE) first column gives prediction second column provides corresponding standard error. classification (.e., EBMClassifier objects)...TBD.","code":""}]
